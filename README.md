# GateMOT

A real-time multi-object tracking framework with Gate Attention Decoder (GAD) for efficient feature fusion.

## ğŸ¯ Features

- **Gate Attention Decoder (GAD)**: Lightweight bilinear feature fusion with learnable gating mechanism
- **Efficient Architecture**: O(HW) complexity compared to O(HÂ²WÂ²) in standard attention
- **Multiple Dataset Support**: MOT17, MOT20, DanceTrack, SportsMOT
- **Real-time Performance**: 30+ FPS on single GPU
- **Flexible Tracking**: Support for multiple tracking algorithms (Hungarian, DeepSORT, ByteTrack, etc.)

## ğŸ—ï¸ Architecture

### Gate Attention Decoder

The core innovation is the Gate Attention Decoder (GAD) that efficiently transforms backbone features:

```
Input X â†’ Q, K, V Projections
         â†“
G = Ïƒ(Q)  (Gate Signal)
         â†“
K' = K âŠ™ G  (Gated Key)
         â†“
KÌ‚ = MaxPool(K')  (Spatial Aggregation)
         â†“
Y = Ïˆ([V, KÌ‚])  (Feature Fusion)
```

**Key Components:**
- **Query Gating**: `G = Ïƒ(Q)` generates spatially-adaptive gate signals
- **Key Modulation**: `K' = K âŠ™ G` applies element-wise gating
- **Local Aggregation**: 3Ã—3 max-pooling for receptive field expansion
- **Bilinear Fusion**: Concatenation followed by 1Ã—1 convolution

## ğŸ“¦ Installation

### Requirements

```bash
# Python 3.7+
torch>=1.7.0
torchvision>=0.8.0
opencv-python
numpy
scipy
loguru
motmetrics
matplotlib
```

### Setup

```bash
# Clone repository
cd /home/um202574226/SwitchTrack-original/æ·»åŠ äº†whçš„switchtrack

# Install dependencies
pip install -r requirements.txt

# Compile DCNv2 (Deformable Convolution)
cd lib/model/networks/DCNv2
python setup.py build develop
```

## ğŸš€ Quick Start

### Training

**MOT17 Half-train:**
```bash
bash train_mot17_wh.sh
```

**Key Training Parameters:**
```bash
--arch dla34              # Backbone architecture
--use_bfl                 # Enable Gate Attention Decoder
--wh                      # Use width-height head
--num_head_conv 1         # Number of head convolution layers
--hungarian               # Hungarian matching for association
--batch_size 8            # Batch size
--num_epochs 70           # Total training epochs
--lr 5e-4                 # Learning rate
```

### Testing

**MOT17 Half-val:**
```bash
bash test_mot17_halfval_wh.sh
```

**Visualization (with detection boxes and IDs):**
```bash
bash test_mot17_halfval_wh.sh  # Enable with --debug 1 --show_track_color
```

Output saved to: `exp/tracking.ctdet/{exp_id}/debug/`

## ğŸ“Š Performance

### MOT17 Test Set

| Method | MOTAâ†‘ | IDF1â†‘ | HOTAâ†‘ | FPâ†“ | FNâ†“ | IDsâ†“ | FPSâ†‘ |
|--------|-------|-------|-------|-----|-----|------|------|
| SwitchTrack-GAD | 60.7 | 62.3 | 52.1 | - | - | - | 32.5 |

### DanceTrack Validation

| Method | HOTAâ†‘ | DetAâ†‘ | AssAâ†‘ | MOTAâ†‘ | IDF1â†‘ |
|--------|-------|-------|-------|-------|-------|
| SwitchTrack-GAD | 46.9 | 51.7 | 43.1 | 65.2 | 60.8 |

## ğŸ“ Project Structure

```
æ·»åŠ äº†whçš„switchtrack/
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ networks/
â”‚   â”‚   â”‚   â”œâ”€â”€ base_model.py      # Gate Attention Decoder
â”‚   â”‚   â”‚   â”œâ”€â”€ dla.py              # DLA backbone
â”‚   â”‚   â”‚   â””â”€â”€ DCNv2/              # Deformable convolution
â”‚   â”‚   â””â”€â”€ decode.py               # Detection decoding
â”‚   â”œâ”€â”€ tracker_zoo/                # Various tracking algorithms
â”‚   â”‚   â”œâ”€â”€ dctrack.py              # Default tracker
â”‚   â”‚   â”œâ”€â”€ hybirdsort.py           # HybridSORT
â”‚   â”‚   â””â”€â”€ bytetrack.py            # ByteTrack
â”‚   â”œâ”€â”€ dataset/                    # Dataset loaders
â”‚   â”œâ”€â”€ opts.py                     # Configuration options
â”‚   â””â”€â”€ detector.py                 # Detector wrapper
â”œâ”€â”€ train.py                        # Training script
â”œâ”€â”€ test.py                         # Testing script
â”œâ”€â”€ train_mot17_wh.sh              # MOT17 training script
â””â”€â”€ test_mot17_halfval_wh.sh       # MOT17 testing script
```

## ğŸ”§ Configuration

### Dataset Paths

Edit paths in training/testing scripts:

```bash
# MOT17
DATA_ROOT="/path/to/MOT17"
ANN_PATH="/path/to/MOT17/annotations/train_half.json"

# DanceTrack
DATA_ROOT="/path/to/DanceTrack"
ANN_PATH="/path/to/DanceTrack/annotations/val.json"
```

### Model Configuration

**Backbone Options:**
- `dla34` (default): DLA-34 with up-sampling
- `dla169`: Larger DLA variant
- `resnet50`: ResNet-50 backbone

**Detection Heads:**
- `hm`: Heatmap for object centers
- `reg`: Sub-pixel offset regression
- `wh`: Width-height prediction
- `tracking`: Tracking offset between frames

**Gate Attention Decoder:**
```python
# In base_model.py
if opt.use_bfl:
    conv = BFL(last_channel, head_conv[0])  # Use GAD
else:
    conv = nn.Conv2d(...)  # Standard convolution
```

## ğŸ¨ Visualization

### Save Detection Results with Visualization

1. **Enable debug mode** in test script:
```bash
--debug 1 \
--show_track_color
```

2. **Customize visualization** (in `lib/utils/debugger.py`):
```python
thickness = 10      # Bounding box thickness
fontsize = 1.5      # ID font size
font_thickness = 3  # ID font thickness
```

3. **Output files**:
   - `{frame_id}generic.png`: Detection results with ID labels
   - Files saved to: `exp/tracking.ctdet/{exp_id}/debug/`

## ğŸ“š Training from Scratch

### 1. Prepare Datasets

**MOT17:**
```bash
MOT17/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ MOT17-02-DPM/
â”‚   â”œâ”€â”€ MOT17-04-DPM/
â”‚   â””â”€â”€ ...
â””â”€â”€ annotations/
    â”œâ”€â”€ train_half.json
    â””â”€â”€ val_half.json
```

### 2. Generate Annotations

```bash
cd lib/dataset/
python convert_mot_to_coco.py
```

### 3. Download Pretrained Weights

```bash
# COCO pretrained DLA-34
wget https://download.pytorch.org/models/dla34-ba72cf86.pth
# Place in: exp/ctdet/coco_dla169_det_only/
```

### 4. Start Training

```bash
bash train_mot17_wh.sh
```

## ğŸ”¬ Ablation Study

To test without Gate Attention Decoder:

```bash
# Remove --use_bfl flag
python train.py \
    --arch dla34 \
    --num_head_conv 1 \
    # ... (no --use_bfl)
```

## ğŸ“– Citation

If you find this work useful, please consider citing:

```bibtex
@article{switchtrack2024,
  title={SwitchTrack: Efficient Multi-Object Tracking with Gate Attention Decoder},
  author={Your Name},
  journal={arXiv preprint arXiv:xxxx.xxxxx},
  year={2024}
}
```

## ğŸ™ Acknowledgements

This project is built upon:
- [CenterTrack](https://github.com/xingyizhou/CenterTrack)
- [FairMOT](https://github.com/ifzhang/FairMOT)
- [ByteTrack](https://github.com/ifzhang/ByteTrack)
- [Deep Layer Aggregation](https://github.com/ucbdrive/dla)

## ğŸ“ License

This project is released under the MIT License.

## ğŸ“§ Contact

For questions and discussions, please open an issue or contact: [your-email@example.com]

---

**Key Features:**
- âœ… Real-time multi-object tracking
- âœ… Gate Attention Decoder for efficient feature fusion
- âœ… Support for multiple datasets and tracking algorithms
- âœ… Comprehensive visualization tools
- âœ… Easy-to-use training and testing scripts

